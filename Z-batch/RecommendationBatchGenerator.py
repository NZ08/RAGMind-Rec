import pandas as pd
import json
from typing import Dict, Any, List
import os
from pathlib import Path
import asyncio
import aiohttp
import time
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class RecommendationBatchGenerator:
    """å•†å“æ¨èæ‰¹å¤„ç†ç”Ÿæˆå™¨
    
    åŸºäºç”¨æˆ·äº¤å‰æ­£è´Ÿæ ·æœ¬æ•°æ®ç”Ÿæˆæ‰¹å¤„ç†è¯·æ±‚ï¼Œç”¨äºå•†å“æ¨èï¼š
    åŸºäºäº¤å‰æ ·æœ¬æ¨èåŸæ•°é‡ä¸€åŠçš„å•†å“
    """
    
    def __init__(self, temperature: float = 0.5):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            temperature: AIæ¨¡å‹çš„æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§
        """
        self.temperature = temperature
        self.api_key = os.getenv('DOUBAO_API_KEY')
        self.base_url = "https://ark.cn-beijing.volces.com/api/v3"
        self.model_endpoint = os.getenv('DOUBAO_MODEL_ENDPOINT')
        self.recommendation_system_prompt = (
            "You are a professional recommendation system assistant, "
            "skilled at analyzing user preference patterns from positive and negative samples "
            "to recommend suitable products."
        )
    
    def _create_recommendation_prompt(self, user_data: Dict[str, Any]) -> str:
        """åˆ›å»ºå•†å“æ¨èçš„æç¤ºè¯æ¨¡æ¿
        
        Args:
            user_data: ç”¨æˆ·æ•°æ®å­—å…¸
            
        Returns:
            æ ¼å¼åŒ–çš„æç¤ºè¯å­—ç¬¦ä¸²
        """
        cross_arranged_items = user_data.get('cross_arranged_items', [])
        total_items = len(cross_arranged_items)
        recommend_count = max(1, total_items // 2)  # æ¨èåŸæ•°é‡ä¸€åŠçš„å•†å“
        
        # æ„å»ºäº¤å‰æ ·æœ¬æ•°æ®
        items_text = "\n".join([f"{i+1}. {item}" for i, item in enumerate(cross_arranged_items)])
        
        prompt = f"""Task: Based on the user's historical product interaction data, analyze user preferences and recommend {recommend_count} products that best match the user's interests.

User's Historical Product Interactions:
{items_text}

Analysis Guidelines:
1. Analyze the user's interaction patterns with different products
2. Identify what types of products, features, categories, and characteristics the user prefers
3. Look for patterns in product attributes that indicate user preferences
4. Consider factors like product categories, features, brands, price ranges, etc.
5. Recommend {recommend_count} products that align with identified user preferences

Requirements:
1. Carefully analyze the user's interaction history to understand preferences
2. Extract key preference indicators from the interaction data
3. Recommend {recommend_count} products with detailed reasoning based on identified patterns
4. Strictly output results in JSON object format, containing the following fields:
   - analysis: Brief analysis of user preferences based on interaction patterns
   - recommended_items: Array of {recommend_count} recommended products, each containing:
     - title: Product title
     - category: Product category
     - key_features: Key features that match user preferences
     - reasoning: Why this product is recommended based on user's interaction patterns

Please ensure the output is in valid JSON format."""
        return prompt
    
    async def _execute_volcengine_batch_inference(self, session: aiohttp.ClientSession, user_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç«å±±æ–¹èˆŸæ‰¹é‡æ¨ç†ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
        
        Args:
            session: aiohttpå®¢æˆ·ç«¯ä¼šè¯
            user_data: ç”¨æˆ·æ•°æ®å­—å…¸
            
        Returns:
            æ¨ç†ç»“æœå­—å…¸
        """
        try:
            # åˆ›å»ºæ¨èæç¤ºè¯
            recommendation_prompt = self._create_recommendation_prompt(user_data)
            
            # æ„å»ºè¯·æ±‚æ¶ˆæ¯
            messages = [
                {
                    "role": "system",
                    "content": self.recommendation_system_prompt
                },
                {
                    "role": "user",
                    "content": recommendation_prompt
                }
            ]
            
            # æ„å»ºæ‰¹é‡æ¨ç†APIè¯·æ±‚
            batch_url = f"{self.base_url.rstrip('/')}/batch/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": self.model_endpoint,
                "messages": messages,
                "response_format": {
                    "type": "json_object"
                },
                "temperature": self.temperature,
                "max_tokens": 1500,
                "thinking": {
                    "type": "disabled"
                },
            }
            
            # å‘é€å¼‚æ­¥è¯·æ±‚
            async with session.post(batch_url, headers=headers, json=payload, timeout=aiohttp.ClientTimeout(total=1800)) as response:
                if response.status == 200:
                    result = await response.json()
                    return {
                        "success": True,
                        "user_id": user_data.get('UserID', 'Unknown'),
                        "result": result,
                        "model": result.get('model', 'N/A'),
                        "usage": result.get('usage', {})
                    }
                else:
                    error_text = await response.text()
                    return {
                        "success": False,
                        "user_id": user_data.get('UserID', 'Unknown'),
                        "error": f"HTTP {response.status}: {error_text}"
                    }
                
        except asyncio.TimeoutError:
            return {
                "success": False,
                "user_id": user_data.get('UserID', 'Unknown'),
                "error": "Request timeout"
            }
        except Exception as e:
            return {
                "success": False,
                "user_id": user_data.get('UserID', 'Unknown'),
                "error": str(e)
            }
    
    async def execute_realtime_batch_inference_async(self, 
                                                    csv_file_path: str,
                                                    fusion_item_path: str,
                                                    output_file: str = "./data/doubao_results.txt",
                                                    max_users: int = None,
                                                    max_concurrent: int = 10) -> bool:
        """æ‰§è¡Œå¼‚æ­¥å®æ—¶æ¨ç†
        
        Args:
            csv_file_path: ç”¨æˆ·äº¤äº’CSVæ–‡ä»¶è·¯å¾„
            fusion_item_path: å•†å“ä¿¡æ¯CSVæ–‡ä»¶è·¯å¾„
            output_file: ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆtxtæ ¼å¼ï¼‰
            max_users: æœ€å¤§å¤„ç†ç”¨æˆ·æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰ç”¨æˆ·
            max_concurrent: æœ€å¤§å¹¶å‘æ•°é‡
            
        Returns:
            æ˜¯å¦æˆåŠŸå®Œæˆæ‰€æœ‰æ¨ç†
        """
        try:
            # éªŒè¯è¾“å…¥æ–‡ä»¶
            csv_path = Path(csv_file_path)
            fusion_path = Path(fusion_item_path)
            if not csv_path.exists():
                raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
            if not fusion_path.exists():
                raise FileNotFoundError(f"å•†å“ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {fusion_item_path}")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # æå–ç”¨æˆ·ç‰¹å¾
            print(f"æ­£åœ¨ä»CSVæ–‡ä»¶æå–ç”¨æˆ·ç‰¹å¾: {csv_file_path}")
            print(f"æ­£åœ¨è¯»å–å•†å“ä¿¡æ¯æ–‡ä»¶: {fusion_item_path}")
            users_data = self._extract_user_features(csv_file_path, fusion_item_path)
            
            total_users = len(users_data)
            if total_users == 0:
                print("è­¦å‘Š: æ²¡æœ‰æå–åˆ°ç”¨æˆ·æ•°æ®")
                return True
            
            # é™åˆ¶å¤„ç†çš„ç”¨æˆ·æ•°é‡
            if max_users is not None and max_users > 0:
                users_data = users_data[:max_users]
                process_users = min(max_users, total_users)
                print(f"æˆåŠŸæå–ç”¨æˆ·ç‰¹å¾ï¼Œå…±{total_users:,}ä¸ªç”¨æˆ·ï¼Œå°†å¤„ç†å‰{process_users}ä¸ªç”¨æˆ·")
            else:
                process_users = total_users
                print(f"æˆåŠŸæå–ç”¨æˆ·ç‰¹å¾ï¼Œå…±{total_users:,}ä¸ªç”¨æˆ·")
            
            print(f"ä½¿ç”¨ç«å±±æ–¹èˆŸæ¨ç†æ¥å…¥ç‚¹: {self.base_url}")
            print(f"æœ€å¤§å¹¶å‘æ•°: {max_concurrent}")
            print("-" * 60)
            
            # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(max_concurrent)
            
            # åˆ›å»ºaiohttpä¼šè¯
            connector = aiohttp.TCPConnector(limit=max_concurrent * 2, limit_per_host=max_concurrent)
            timeout = aiohttp.ClientTimeout(total=1800)  # 30åˆ†é’Ÿè¶…æ—¶
            
            async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
                # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡åˆ—è¡¨
                tasks = []
                for i, user_data in enumerate(users_data):
                    task = self._process_single_user_async(session, semaphore, user_data, i+1, process_users)
                    tasks.append(task)
                
                # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # å†™å…¥ç»“æœåˆ°æ–‡ä»¶
                with open(output_file, 'w', encoding='utf-8') as f:
                    for result in results:
                        if isinstance(result, Exception):
                            error_result = {
                                "success": False,
                                "user_id": "Unknown",
                                "error": str(result)
                            }
                            f.write(json.dumps(error_result, ensure_ascii=False) + '\n')
                        else:
                            f.write(json.dumps(result, ensure_ascii=False) + '\n')
            
            print(f"\nğŸ‰ å¼‚æ­¥æ¨ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_path.absolute()}")
            return True
            
        except FileNotFoundError as e:
            print(f"âŒ æ–‡ä»¶é”™è¯¯: {e}")
            return False
        except Exception as e:
            print(f"âŒ æ‰§è¡Œå¼‚æ­¥æ¨ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    async def _process_single_user_async(self, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore, 
                                       user_data: Dict[str, Any], index: int, total: int) -> Dict[str, Any]:
        """å¼‚æ­¥å¤„ç†å•ä¸ªç”¨æˆ·
        
        Args:
            session: aiohttpå®¢æˆ·ç«¯ä¼šè¯
            semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
            user_data: ç”¨æˆ·æ•°æ®
            index: å½“å‰ç”¨æˆ·ç´¢å¼•
            total: æ€»ç”¨æˆ·æ•°
            
        Returns:
            å¤„ç†ç»“æœ
        """
        async with semaphore:
            user_id = user_data.get('UserID', f'User_{index}')
            print(f"å¤„ç†ç”¨æˆ· {user_id} ({index}/{total})... ", end="")
            
            # æ‰§è¡Œæ¨ç†
            result = await self._execute_volcengine_batch_inference(session, user_data)
            
            if result['success']:
                print("âœ“ æˆåŠŸ")
            else:
                print(f"âœ— å¤±è´¥: {result['error']}")
            
            return result
    
    def execute_realtime_batch_inference(self, 
                                       csv_file_path: str,
                                       fusion_item_path: str,
                                       output_file: str = "./data/doubao_results.txt",
                                       max_users: int = None) -> bool:
        """æ‰§è¡Œå®æ—¶æ¨ç†
        
        Args:
            csv_file_path: ç”¨æˆ·äº¤äº’CSVæ–‡ä»¶è·¯å¾„
            fusion_item_path: å•†å“ä¿¡æ¯CSVæ–‡ä»¶è·¯å¾„
            output_file: ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆtxtæ ¼å¼ï¼‰
            max_users: æœ€å¤§å¤„ç†ç”¨æˆ·æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰ç”¨æˆ·
            
        Returns:
            æ˜¯å¦æˆåŠŸå®Œæˆæ‰€æœ‰æ¨ç†
        """
        try:
            # éªŒè¯è¾“å…¥æ–‡ä»¶
            csv_path = Path(csv_file_path)
            fusion_path = Path(fusion_item_path)
            if not csv_path.exists():
                raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}")
            if not fusion_path.exists():
                raise FileNotFoundError(f"å•†å“ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {fusion_item_path}")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # æå–ç”¨æˆ·ç‰¹å¾
            print(f"æ­£åœ¨ä»CSVæ–‡ä»¶æå–ç”¨æˆ·ç‰¹å¾: {csv_file_path}")
            print(f"æ­£åœ¨è¯»å–å•†å“ä¿¡æ¯æ–‡ä»¶: {fusion_item_path}")
            users_data = self._extract_user_features(csv_file_path, fusion_item_path)
            
            total_users = len(users_data)
            if total_users == 0:
                print("è­¦å‘Š: æ²¡æœ‰æå–åˆ°ç”¨æˆ·æ•°æ®")
                return True
            
            # é™åˆ¶å¤„ç†çš„ç”¨æˆ·æ•°é‡
            if max_users is not None and max_users > 0:
                users_data = users_data[:max_users]
                process_users = min(max_users, total_users)
                print(f"æˆåŠŸæå–ç”¨æˆ·ç‰¹å¾ï¼Œå…±{total_users:,}ä¸ªç”¨æˆ·ï¼Œå°†å¤„ç†å‰{process_users}ä¸ªç”¨æˆ·")
            else:
                process_users = total_users
                print(f"æˆåŠŸæå–ç”¨æˆ·ç‰¹å¾ï¼Œå…±{total_users:,}ä¸ªç”¨æˆ·")
            
            print(f"ä½¿ç”¨ç«å±±æ–¹èˆŸæ¨ç†æ¥å…¥ç‚¹: {self.base_url}")

            print("-" * 60)
            
            # æ‰“å¼€è¾“å‡ºæ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                # å¤„ç†æ¯ä¸ªç”¨æˆ·
                for i, user_data in enumerate(users_data):
                    user_id = user_data.get('UserID', f'User_{i+1}')
                    print(f"å¤„ç†ç”¨æˆ· {user_id} ({i+1}/{process_users})... ", end="")
                    
                    # æ‰§è¡Œæ¨ç†
                    result = self._execute_volcengine_batch_inference(user_data)
                    
                    # ç›´æ¥å°†ç»“æœå†™å…¥txtæ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªç»“æœ
                    f.write(json.dumps(result, ensure_ascii=False) + '\n')
                    
                    if result['success']:
                        print("âœ“ æˆåŠŸ")
                    else:
                        print(f"âœ— å¤±è´¥: {result['error']}")
                    
                    # å°å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(0.1)
            
            print(f"\nğŸ‰ æ¨ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_path.absolute()}")
            return True
            
        except FileNotFoundError as e:
            print(f"âŒ æ–‡ä»¶é”™è¯¯: {e}")
            return False
        except Exception as e:
            print(f"âŒ æ‰§è¡Œæ¨ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _extract_user_features(self, csv_file_path: str, fusion_item_path: str) -> List[Dict[str, Any]]:
        """ä»CSVæ–‡ä»¶ä¸­æå–ç”¨æˆ·ç‰¹å¾ï¼ŒåŒ…æ‹¬æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬
        
        Args:
            csv_file_path: ç”¨æˆ·äº¤äº’CSVæ–‡ä»¶è·¯å¾„
            fusion_item_path: å•†å“ä¿¡æ¯CSVæ–‡ä»¶è·¯å¾„
            
        Returns:
            æå–çš„ç”¨æˆ·ç‰¹å¾åˆ—è¡¨
        """
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_file_path)
        
        # è¯»å–Fusion_Item.csvæ–‡ä»¶ï¼Œå»ºç«‹asinåˆ°å•†å“ä¿¡æ¯çš„æ˜ å°„
        fusion_df = pd.read_csv(fusion_item_path)
        asin_to_item = {}
        for _, row in fusion_df.iterrows():
            asin = row['asin']
            asin_to_item[asin] = {
                'content': str(row['content']) if pd.notna(row['content']) else "",
                'brief_description': str(row['brief_description']) if pd.notna(row['brief_description']) else "",
                'title': str(row['title']) if pd.notna(row['title']) else ""
            }
        
        # å­˜å‚¨æå–ç»“æœçš„åˆ—è¡¨
        extracted_data = []
        
        for index, row in df.iterrows():
            user_id = row['UserID']
            
            # æå–æ­£æ ·æœ¬contentå­—æ®µï¼ˆç”¨ç®¡é“ç¬¦åˆ†éš”çš„JSONå­—ç¬¦ä¸²ï¼‰
            pos_content_raw = row['content']
            pos_content_elements = []
            if pd.notna(pos_content_raw):
                # æŒ‰ç®¡é“ç¬¦åˆ†å‰²content
                content_parts = pos_content_raw.split('|')
                for part in content_parts:
                    try:
                        # å°è¯•è§£æJSON
                        parsed_content = json.loads(part)
                        pos_content_elements.append(parsed_content)
                    except json.JSONDecodeError:
                        # å¦‚æœä¸æ˜¯æœ‰æ•ˆJSONï¼Œç›´æ¥ä¿å­˜åŸå§‹å­—ç¬¦ä¸²
                        pos_content_elements.append(part.strip())
            
            # æå–æ­£æ ·æœ¬brief_descriptionå­—æ®µï¼ˆç”¨ç®¡é“ç¬¦åˆ†éš”ï¼‰
            pos_brief_desc_raw = row['brief_description']
            pos_brief_desc_elements = []
            if pd.notna(pos_brief_desc_raw):
                # æŒ‰ç®¡é“ç¬¦åˆ†å‰²brief_description
                pos_brief_desc_elements = [desc.strip() for desc in pos_brief_desc_raw.split('|')]
            
            # æå–æ­£æ ·æœ¬titleå­—æ®µï¼ˆç”¨ç®¡é“ç¬¦åˆ†éš”ï¼‰
            pos_title_raw = row['title']
            pos_title_elements = []
            if pd.notna(pos_title_raw):
                # æŒ‰ç®¡é“ç¬¦åˆ†å‰²title
                pos_title_elements = [title.strip() for title in pos_title_raw.split('|')]
            
            # æå–negå­—æ®µä¸­çš„asinï¼ˆç”¨ç®¡é“ç¬¦åˆ†éš”ï¼‰
            neg_raw = row['neg']
            neg_asins = []
            if pd.notna(neg_raw):
                # æŒ‰ç®¡é“ç¬¦åˆ†å‰²neg
                neg_asins = [neg.strip() for neg in neg_raw.split('|')]
            
            # å»é™¤æ¯ä¸ªç‰¹å¾çš„æœ€åä¸€ä¸ªå…ƒç´ ï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰
            if len(pos_content_elements) > 0:
                pos_content_elements = pos_content_elements[:-1]
            if len(pos_brief_desc_elements) > 0:
                pos_brief_desc_elements = pos_brief_desc_elements[:-1]
            if len(pos_title_elements) > 0:
                pos_title_elements = pos_title_elements[:-1]
            if len(neg_asins) > 0:
                neg_asins = neg_asins[:-1]
            
            # æ ¹æ®neg_asinsæŸ¥æ‰¾è´Ÿæ ·æœ¬ç‰¹å¾
            neg_content_elements = []
            neg_brief_desc_elements = []
            neg_title_elements = []
            
            for asin in neg_asins:
                if asin in asin_to_item:
                    item_info = asin_to_item[asin]
                    
                    # è´Ÿæ ·æœ¬contentç‰¹å¾
                    if item_info['content']:
                        try:
                            parsed_content = json.loads(item_info['content'])
                            neg_content_elements.append(parsed_content)
                        except json.JSONDecodeError:
                            neg_content_elements.append(item_info['content'])
                    else:
                        neg_content_elements.append("No Data")
                    
                    # è´Ÿæ ·æœ¬brief_descriptionç‰¹å¾
                    if item_info['brief_description']:
                        neg_brief_desc_elements.append(item_info['brief_description'])
                    else:
                        neg_brief_desc_elements.append("No Data")
                    
                    # è´Ÿæ ·æœ¬titleç‰¹å¾
                    if item_info['title']:
                        neg_title_elements.append(item_info['title'])
                    else:
                        neg_title_elements.append("No Data")
            
            # ç›´æ¥æ„å»ºäº¤å‰æ’åˆ—çš„å­—ç¬¦ä¸²
            cross_arranged_items = []
            min_length = min(len(pos_content_elements), len(neg_content_elements))
            for i in range(min_length):
                # æ­£æ ·æœ¬é¡¹
                pos_item = f"title: {pos_title_elements[i]}, Brief Description: {pos_brief_desc_elements[i]}, content: {pos_content_elements[i]}"
                # è´Ÿæ ·æœ¬é¡¹
                neg_item = f"title: {neg_title_elements[i]}, Brief Description: {neg_brief_desc_elements[i]}, content: {neg_content_elements[i]}"
                # äº¤å‰æ’åˆ—
                cross_arranged_items.append(f"Item {i*2+1} {pos_item}")
                cross_arranged_items.append(f"Item {i*2+2} {neg_item}")
            
            # æ„å»ºç”¨æˆ·ç‰¹å¾æ•°æ®ï¼ˆåªä¿ç•™å¿…è¦çš„äº¤å‰æ’åˆ—æ•°æ®ï¼‰
            user_features = {
                'UserID': user_id,
                'cross_arranged_items': cross_arranged_items
            }
            
            extracted_data.append(user_features)
        
        return extracted_data

async def main_async():
    """å¼‚æ­¥ä¸»å‡½æ•° - æ‰§è¡Œè±†åŒ…å¼‚æ­¥æ¨ç†"""
    # é…ç½®å‚æ•°
    config = {
        'csv_file_path': "meta-data/user_interactions.csv",
        'fusion_item_path': "meta-data/Fusion_Item.csv",
        'temperature': 0.7,
        'output_file': "./data/doubao_results_async.txt",
        'max_concurrent': 10
    }
    
    print("=" * 60)
    print("ğŸš€ å•†å“æ¨èè±†åŒ…å¼‚æ­¥æ¨ç†")
    print("=" * 60)
    print(f"ç”¨æˆ·äº¤äº’æ–‡ä»¶: {config['csv_file_path']}")
    print(f"å•†å“ä¿¡æ¯æ–‡ä»¶: {config['fusion_item_path']}")
    print(f"æ¸©åº¦å‚æ•°: {config['temperature']}")
    print(f"æœ€å¤§å¹¶å‘æ•°: {config['max_concurrent']}")
    print(f"ç»“æœè¾“å‡º: {config['output_file']}")
    print("-" * 60)
    
    try:
        generator = RecommendationBatchGenerator(temperature=config['temperature'])
        
        # æ£€æŸ¥APIé…ç½®
        if not generator.api_key or not generator.model_endpoint:
            print("\nâŒ APIé…ç½®ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")
            return
        
        print("\nâœ“ APIé…ç½®å·²éªŒè¯")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æ‰§è¡Œå¼‚æ­¥æ¨ç†
        success = await generator.execute_realtime_batch_inference_async(
            csv_file_path=config['csv_file_path'],
            fusion_item_path=config['fusion_item_path'],
            output_file=config['output_file'],
            max_users=50,
            max_concurrent=config['max_concurrent']
        )
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        async_duration = end_time - start_time
        
        if success:
            print(f"\nâœ… å¼‚æ­¥æ¨ç†å®Œæˆï¼è€—æ—¶: {async_duration:.2f}ç§’")
        else:
            print("\nâŒ å¼‚æ­¥æ¨ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
            
        # æ€§èƒ½å¯¹æ¯”è¯´æ˜
        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½ä¼˜åŒ–è¯´æ˜:")
        print(f"â€¢ å¼‚æ­¥å¹¶å‘å¤„ç†å¯æ˜¾è‘—å‡å°‘æ€»è€—æ—¶")
        print(f"â€¢ å½“å‰å¹¶å‘æ•°è®¾ç½®: {config['max_concurrent']}")
        print(f"â€¢ å¯é€šè¿‡è°ƒæ•´max_concurrentå‚æ•°æ§åˆ¶å¹¶å‘æ•°é‡")
        print(f"â€¢ å»ºè®®æ ¹æ®APIé™åˆ¶å’Œç½‘ç»œæ¡ä»¶é€‰æ‹©åˆé€‚çš„å¹¶å‘æ•°")
        print("=" * 60)
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")

def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œè±†åŒ…å®æ—¶æ¨ç†ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œä¿ç•™ç”¨äºå¯¹æ¯”ï¼‰"""
    # é…ç½®å‚æ•°
    config = {
        'csv_file_path': "meta-data/user_interactions.csv",
        'fusion_item_path': "meta-data/Fusion_Item.csv",
        'temperature': 0.7,
        'output_file': "./data/doubao_results_sync.txt"
    }
    
    print("=" * 60)
    print("ğŸš€ å•†å“æ¨èè±†åŒ…åŒæ­¥æ¨ç†ï¼ˆå¯¹æ¯”ç‰ˆæœ¬ï¼‰")
    print("=" * 60)
    print(f"ç”¨æˆ·äº¤äº’æ–‡ä»¶: {config['csv_file_path']}")
    print(f"å•†å“ä¿¡æ¯æ–‡ä»¶: {config['fusion_item_path']}")
    print(f"æ¸©åº¦å‚æ•°: {config['temperature']}")
    print(f"ç»“æœè¾“å‡º: {config['output_file']}")
    print("-" * 60)
    
    try:
        generator = RecommendationBatchGenerator(temperature=config['temperature'])
        
        # æ£€æŸ¥APIé…ç½®
        if not generator.api_key or not generator.model_endpoint:
            print("\nâŒ APIé…ç½®ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")
            return
        
        print("\nâœ“ APIé…ç½®å·²éªŒè¯")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        success = generator.execute_realtime_batch_inference(
            csv_file_path=config['csv_file_path'],
            fusion_item_path=config['fusion_item_path'],
            output_file=config['output_file'],
            max_users=5
        )
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        sync_duration = end_time - start_time
        
        if success:
            print(f"\nâœ… åŒæ­¥æ¨ç†å®Œæˆï¼è€—æ—¶: {sync_duration:.2f}ç§’")
        else:
            print("\nâŒ åŒæ­¥æ¨ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")


if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥ç‰ˆæœ¬
    asyncio.run(main_async())